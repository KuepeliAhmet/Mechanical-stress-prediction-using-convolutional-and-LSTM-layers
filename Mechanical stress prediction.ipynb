{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regression for Boundary Value Problems in Solid Mechanics\n",
    "\n",
    "Authors: Vivek Chavan (Github: Vivek9Chavan), Chi Shing Li (Github: charles4444), Ahmet KÃ¼peli\n",
    "\n",
    "Date: 15.01.2020\n",
    "\n",
    "This Script was created for the course Computational Intelligence in Engineering hold by Arndt Koeppe and Marion Mundt, supervised by Prof. Bernd Markert @ Institue of General Mechanics RWTH Aachen University "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tensorflow gpu check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name())) \n",
    "else: print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPool2D, TimeDistributed, LSTM\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from helpers import DataReader, plot_sample\n",
    "\n",
    "#Importing all functions from metrics, just in case:\n",
    "from metrics import mean_absolute_percentage_error, max_relative_error, symmetric_mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE CHECK THE PATH\n",
    "tfrecord_dir = os.path.join(r'C:_path', 'data', 'tfrecord')\n",
    "tfrecord_files = glob.glob(os.path.join(tfrecord_dir, '*'))\n",
    "#Data\n",
    "data_format = 'NSXYF'     # Axes in data: Samples (N), Sequence (S), X-Axis (X), Y-Axis (Y), Features (F)\n",
    "data_shape = [None, 100, 9, 9, 21]\n",
    "idx = {'Xi_boundary': [0, 1],\n",
    "       'Ui': [8, 9],\n",
    "       'Fi': [16, 17],\n",
    "       'boundary': [4],\n",
    "       'Xi': [6, 7],\n",
    "       'Ui_boundary': [2, 3],\n",
    "       'body': [5],\n",
    "       'Si': [10, 11, 12, 13]}\n",
    "input_features = idx['Ui'] + idx['Si']  \n",
    "output_features = idx['Fi']\n",
    "\n",
    "\n",
    "# Hyper parameters - Batch sizes & Learning rates\n",
    "learning_rate=[0.001]\n",
    "batch_size_loop=[16]\n",
    "#learning_rate=[0.0001, 0.001,0.01,0.1]\n",
    "#batch_size_loop=[8,16,32,64,128]\n",
    "\n",
    "#data split\n",
    "tf.random.set_seed(123)\n",
    "tfrecord_files=tf.random.shuffle(tfrecord_files)\n",
    "\n",
    "'Data Split is approx. 70-15-15'\n",
    "train_data, valid_data, test_data = tfrecord_files[:45000], tfrecord_files[45000:55000], tfrecord_files[55000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Mean and SD for hardcoding\n",
    "\n",
    "##  Do not run it unless necessary. The computation takes long time! The Datasets will also need to be plotted before running this.\n",
    "\n",
    "Following is an example of how the Mean and the SD can be calculated for a given feature. Here it will be calculated for the input feature only!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def mean_fun(x):\n",
    "    x_abs = tf.abs(x)\n",
    "    full_mask = tf.cast(tf.sign(x_abs), tf.bool)\n",
    "    sequence_mask = tf.cast(tf.reduce_any(full_mask, axis=(2, 3, 4)), tf.float32)\n",
    "    mean, var = tf.nn.weighted_moments(x, frequency_weights=tf.cast(full_mask, tf.float32), axes=(0, 1, 2, 3))\n",
    "    std = tf.sqrt(var)\n",
    "    x1 = mean\n",
    "    return x1\n",
    "\n",
    "def std_fun(x):\n",
    "    x_abs = tf.abs(x)\n",
    "    full_mask = tf.cast(tf.sign(x_abs), tf.bool)\n",
    "    mean, var = tf.nn.weighted_moments(x, frequency_weights=tf.cast(full_mask, tf.float32), axes=(0, 1, 2, 3))\n",
    "    std = tf.sqrt(var)\n",
    "    x = std\n",
    "    return x\n",
    "\n",
    "def preprocess_output0(y):\n",
    "    mean = tf.convert_to_tensor([0.0, 0.0])\n",
    "    std = tf.convert_to_tensor([1.0, 1.0])\n",
    "    y = (y - mean) / std\n",
    "    return y\n",
    "\n",
    "def mean_and_dummy(*batch):\n",
    "    return (mean_fun(batch[0]), preprocess_output0(batch[1]))\n",
    "\n",
    "def std_and_dummy(*batch):\n",
    "    return (std_fun(batch[0]), preprocess_output0(batch[1]))\n",
    "\n",
    "#not sure if the data is getting reshuffled again!\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "mean_set = train_dataset.map(mean_and_dummy)\n",
    "\n",
    "std_set = train_dataset.map(std_and_dummy)\n",
    "\n",
    "summe1=0\n",
    "sumsd1=0\n",
    "tf0=0\n",
    "\n",
    "#VARY THIS TO CHANGE THE NUMBER OF BATCHES OVER WHICH IT IS CALCULATED\n",
    "no_batches=1\n",
    "\n",
    "# Final Mean Calculation\n",
    "\n",
    "for batch in mean_set.take(no_batches):\n",
    "    summe=sum(batch[0])\n",
    "    #print(batch[0])\n",
    "    summe1=summe+summe1\n",
    "    #tfsum= tf.math.reduce_mean(batch[0], keepdims=False)\n",
    "    #print('Tensorflow mean is:')\n",
    "    #print(tfsum)\n",
    "\n",
    "    \n",
    "for batch in std_set.take(no_batches):\n",
    "    sumsd=sum(batch[0])\n",
    "    #print(batch[0])\n",
    "    sumsd1=sumsd+sumsd1\n",
    "\n",
    "#print(summe1)\n",
    "mean_value=summe1/(no_batches*2)\n",
    "print('The mean is:')\n",
    "print(mean_value)\n",
    "\n",
    "# Final Std calculation\n",
    "#print(sumsd1)\n",
    "std_value=sumsd1/(no_batches*2)\n",
    "print('The SD is:')\n",
    "print(std_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Mean and SD:\n",
    "\n",
    "After running the calculations the final valies for stress (Si) are:\n",
    "\n",
    "##  Ui\n",
    "Mean= 0\n",
    "\n",
    "SD = 0.001\n",
    "\n",
    "##  Si\n",
    "Mean= 0.011196767\n",
    "\n",
    "SD = 4.3331237\n",
    "\n",
    "##  Fi\n",
    "Mean= 0\n",
    "\n",
    "SD = 0.37\n",
    "\n",
    "## These must now be used for hardcoding in the preprocessing below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocessing: THIS IS CORRECT PREPROCESSING!!\n",
    "def preprocess_input(x):\n",
    "    # mean = tf.reduce_mean(x, axis=(0, 1, 2, 3), keepdims=True)\n",
    "    mean = tf.convert_to_tensor([0, 0, 0.011196767, 0.011196767, 0.011196767, 0.011196767], np.float32)\n",
    "    std = tf.convert_to_tensor([0.001, 0.001, 4.3331237, 4.3331237, 4.3331237, 4.3331237], np.float32)\n",
    "    x = (x - mean) / std\n",
    "    return x\n",
    "\n",
    "def preprocess_output(y):\n",
    "    mean = tf.convert_to_tensor([0, 0], np.float32)\n",
    "    std = tf.convert_to_tensor([0.37, 0.37], np.float32)\n",
    "    y = (y - mean) / std\n",
    "    return y\n",
    "\n",
    "def preprocess_fun(*batch):\n",
    "    return (preprocess_input(batch[0]), preprocess_output(batch[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Architecture - AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference Architecture - AlexNet\n",
    "history_list=[]\n",
    "test_loss_list=[]\n",
    "kernel_size=[2,2]\n",
    "epochs=80\n",
    "\n",
    "# for loops - Batch sizes\n",
    "for i in range(0,len(batch_size_loop)):\n",
    "    data_reader = DataReader(data_shape, data_format,  batch_size=batch_size_loop[i], src_type='file',\n",
    "                         slice_tensors=False, buffer_size=1000,\n",
    "                         input_features=input_features, output_features=output_features,\n",
    "                         sample_dtype=tf.float64, cast_dtype=tf.float32)\n",
    "    \n",
    "    train_dataset = data_reader.generate_batch_dataset(train_data, batch_size_loop[i])\n",
    "    valid_dataset = data_reader.generate_batch_dataset(valid_data, batch_size_loop[i])\n",
    "    test_dataset = data_reader.generate_test_dataset(test_data, batch_size_loop[i])\n",
    "    processed_train_dataset = train_dataset.map(preprocess_fun)\n",
    "    processed_valid_dataset = valid_dataset.map(preprocess_fun)\n",
    "    processed_test_dataset = test_dataset.map(preprocess_fun)\n",
    "    \n",
    "    # for loops - Learning rates\n",
    "    for j in range(0,len(learning_rate)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        def create_AlexNET_func(kernel_size,batch_size_loop):\n",
    "            # Input layer:\n",
    "            input_shape = list(data_shape[1:-1]) + [len(input_features)]\n",
    "            inputs = tf.keras.Input(input_shape,batch_size=batch_size_loop)\n",
    "            x = inputs\n",
    "            \n",
    "            # Hidden layers:\n",
    "            #1st Convolutional Layer\n",
    "            x=TimeDistributed(Conv2D(filters=4, kernel_size=kernel_size, padding='same', activation='relu'))(x)\n",
    "            x=TimeDistributed(MaxPool2D(pool_size=(2,2), padding='valid'))(x)\n",
    "    \n",
    "            #2nd Conv. Layer\n",
    "            x = TimeDistributed(Conv2D(filters=10, kernel_size=kernel_size, padding='same', activation='relu'))(x)\n",
    "            x=TimeDistributed(MaxPool2D(pool_size=(2,2), padding='valid'))(x)\n",
    "    \n",
    "            #3rd Conv. Layer\n",
    "            x= TimeDistributed(Conv2D(filters=15,  kernel_size=kernel_size, padding='same', activation='relu'))(x)\n",
    "\n",
    "            #Flattening\n",
    "            x=TimeDistributed(Flatten())(x)\n",
    "    \n",
    "\n",
    "            #1st Dense Layer\n",
    "            x=TimeDistributed(Dense(240, activation='relu'))(x)\n",
    "    \n",
    "            #2nd Dense Layer\n",
    "            x=TimeDistributed(Dense(162, activation='linear'))(x)\n",
    "            x = tf.keras.layers.TimeDistributed(tf.keras.layers.Reshape((9,9,len(output_features))))(x)\n",
    "            #x = tf.keras.layers.Masking(mask_value=0.0)(x)\n",
    "            \n",
    "            # Output layer\n",
    "            return tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        #Training\n",
    "        validation_steps= len(valid_data) / (batch_size_loop[i]*epochs)\n",
    "        steps_per_epoch= len(train_data) / (batch_size_loop[i]* epochs)\n",
    "\n",
    "        model=create_AlexNET_func(kernel_size,batch_size_loop[i])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate[j])\n",
    "        model.compile(loss='mae', optimizer=opt)\n",
    "        history=model.fit(processed_train_dataset,validation_data=processed_valid_dataset,validation_steps=validation_steps,shuffle=True ,steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "        history_list.append(history)\n",
    "        \n",
    "        #Testing\n",
    "        test_loss=model.evaluate(processed_test_dataset)\n",
    "        test_loss_list.append(\"Batch= \" +str(batch_size_loop[i])+ \"; Learning= \" + str(learning_rate[j])+ \"; Test Loss= \" + str(test_loss))\n",
    "        model.save(\"AlexNET_Model_15_Jan\" + str(i) + \".h5\")\n",
    "\n",
    "        \n",
    "        #Vizualisation - traing loss vs validation loss\n",
    "        #plt.title('AlexNet Losses')\n",
    "        #plt.ylabel('Loss')\n",
    "        #plt.xlabel('Epoch')        \n",
    "        #plt.plot(history.history['loss'])\n",
    "        #plt.plot(history.history['val_loss'])\n",
    "        #plt.legend(['Training','Validation'],loc='upper left')\n",
    "        #plt.grid(axis='both',which='major')\n",
    "        #plt.savefig('Fig_alexFinal_B' + str(batch_size_loop[i]) + \"l\" + str(learning_rate[j]) + \".png\")        \n",
    "        #plt.clf()\n",
    "\n",
    "\n",
    "print(test_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Architecture - LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Architecture - LSTM RNN\n",
    "history_list=[]\n",
    "test_loss_list=[]\n",
    "epochs=80\n",
    "\n",
    "# for loops - Batch sizes\n",
    "for i in range(0,len(batch_size_loop)):\n",
    "    data_reader = DataReader(data_shape, data_format,  batch_size=batch_size_loop[i], src_type='file',\n",
    "                         slice_tensors=False, buffer_size=1000,\n",
    "                         input_features=input_features, output_features=output_features,\n",
    "                         sample_dtype=tf.float64, cast_dtype=tf.float32)\n",
    "    \n",
    "    train_dataset = data_reader.generate_batch_dataset(train_data, batch_size_loop[i])\n",
    "    valid_dataset = data_reader.generate_batch_dataset(valid_data, batch_size_loop[i])\n",
    "    test_dataset = data_reader.generate_test_dataset(test_data, batch_size_loop[i])\n",
    "    processed_train_dataset = train_dataset.map(preprocess_fun)\n",
    "    processed_valid_dataset = valid_dataset.map(preprocess_fun)\n",
    "    processed_test_dataset = test_dataset.map(preprocess_fun)\n",
    "    \n",
    "    # for loops - Learning rates\n",
    "    for j in range(0,len(learning_rate)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        def create_RecNET(batch_size):\n",
    "            input_shape = list(data_shape[1:-1]) + [len(input_features)]\n",
    "            \n",
    "            # Input layer\n",
    "            inputs = tf.keras.Input(input_shape,batch_size=batch_size)\n",
    "            x = inputs\n",
    "            \n",
    "            # Hidden layers:\n",
    "            # Flatten layer - flatten 5d tensor into 3d tensor which is the required input for lstm layer\n",
    "            x = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(x)  \n",
    "            #x = SequenceMasking(mask_value=0.)(x)\n",
    "    \n",
    "            # 1st LSTM layer\n",
    "            x = tf.keras.layers.LSTM(324, activation='tanh', return_sequences=True, dropout=0.1)(x)\n",
    "   \n",
    "            # 2nd LSTM layer\n",
    "            x = tf.keras.layers.LSTM(216, activation='tanh', return_sequences=True, dropout=0.1)(x)\n",
    "        \n",
    "            # 3rd LSTM layer\n",
    "            #x = tf.keras.layers.LSTM(216, activation='tanh', return_sequences=True, dropout=0.1)(x)\n",
    "   \n",
    "\n",
    "            #x=tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(334, activation='relu'))(x)\n",
    "            #x= Dropout(0.2)(x)\n",
    "\n",
    "            # Dense layer - linear activation - make sure all output between +ve and -ve\n",
    "            x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(162, activation='linear'))(x)\n",
    "            # Reshape layer - reshape 3d tensor back to 5d tensor\n",
    "            x = tf.keras.layers.TimeDistributed(tf.keras.layers.Reshape((9,9,len(output_features))))(x)\n",
    "            #x = tf.keras.layers.Masking(mask_value=0.)(x)\n",
    "            \n",
    "            # Output layers\n",
    "            return tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        # Training\n",
    "        validation_steps= len(valid_data) / (batch_size_loop[i]*epochs)\n",
    "        steps_per_epoch= len(train_data) / (batch_size_loop[i]* epochs)\n",
    "\n",
    "        model=create_RecNET(batch_size_loop[i])\n",
    "        model.summary()\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate[j])\n",
    "        model.compile(loss='mae', optimizer=opt)\n",
    "        history=model.fit(processed_train_dataset,validation_data=processed_valid_dataset,validation_steps=validation_steps,shuffle=True ,steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "        history_list.append(history)\n",
    "        \n",
    "        #Testing\n",
    "        test_loss=model.evaluate(processed_test_dataset)\n",
    "        test_loss_list.append(\"Batch= \" +str(batch_size_loop[i])+ \"; Learning= \" + str(learning_rate[j])+ \"; Test Loss= \" + str(test_loss))\n",
    "        #model.save(\"RecNET_Model_15_Jan\" + str(i) + \".h5\")\n",
    "        \n",
    "        #Vizualisation - traing loss vs validation loss\n",
    "        #plt.title('RecNet Losses')\n",
    "        #plt.ylabel('Loss')\n",
    "        #plt.xlabel('Epoch')        \n",
    "        #plt.plot(history.history['loss'])\n",
    "        #plt.plot(history.history['val_loss'])\n",
    "        #plt.legend(['Training','Validation'],loc='upper left')\n",
    "        #plt.grid(axis='both',which='major')\n",
    "        #plt.savefig('Fig_RecFinal_B' + str(batch_size_loop[i]) + \"l\" + str(learning_rate[j]) + \".png\")        \n",
    "        #plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Numpy array for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions will be used for generating a target set\n",
    "\n",
    "def dummy_fun_ip(x):\n",
    "    mean = tf.convert_to_tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    std = tf.convert_to_tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "    x=(x-mean)/std\n",
    "    return x\n",
    "\n",
    "def dummy_fun_op(x):\n",
    "    mean = tf.convert_to_tensor([0.0, 0.0])\n",
    "    std = tf.convert_to_tensor([1.0, 1.0])\n",
    "    x=(x-mean)/std\n",
    "    return x    \n",
    "\n",
    "def preprocess_tf(*batch):\n",
    "    return (dummy_fun_ip(batch[0]), dummy_fun_op(batch[1]))\n",
    "\n",
    "processed_test_dataset2 = test_dataset.map(preprocess_tf)\n",
    "\n",
    "nn_target = [batch[1] for batch in processed_test_dataset2]\n",
    "target = zip(*nn_target)\n",
    "target = [np.concatenate(o, axis=0) for o in target]\n",
    "\n",
    "# Get Target set maskm for calculating SMAPPE:\n",
    "target_mask=tf.abs(target)\n",
    "target_mask= tf.cast(tf.sign(target), tf.bool)\n",
    "\n",
    "target2=tf.boolean_mask(target, target_mask)\n",
    "    \n",
    "np.shape(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_output(y_):\n",
    "    mean = tf.convert_to_tensor([0, 0], np.float32)\n",
    "    std = tf.convert_to_tensor([0.37, 0.37], np.float32)\n",
    "    y_ = (y_ * std) + mean\n",
    "    return y_\n",
    "\n",
    "#Creating a similar postprocessing function\n",
    "def postprocess_fun(*batch):\n",
    "    return (postprocess_output(batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAPE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_output = [model.predict(batch) for batch in processed_test_dataset]\n",
    "output = [postprocess_fun(nno) for nno in nn_output]\n",
    "output = zip(*output)\n",
    "output = [np.concatenate(o, axis=0) for o in output]\n",
    "np.shape(output)\n",
    "\n",
    "#Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "output2=tf.boolean_mask(output, target_mask)\n",
    "\n",
    "smape = symmetric_mean_absolute_percentage_error(\n",
    "target2, output2, reduction_axes=None, norm_mode='range')\n",
    "\n",
    "print('The SMAPE is: ')\n",
    "print(smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Target\n",
    "for iter in range(20):\n",
    "    \n",
    "    sample = target[3]\n",
    "    tmp2 = sample[0+(500*iter):500+(500*iter), 0, 0, 0]\n",
    "    print(tmp2.shape)\n",
    "    fig = plt.figure(figsize=(7,3))\n",
    "    plt.plot(tmp2, 'r')\n",
    "\n",
    "    sample3 = output[3]\n",
    "    tmp3 = sample3[0+(500*iter):500+(500*iter), 0, 0, 0]\n",
    "    plt.plot(tmp3, 'g')\n",
    "    plt.legend(['Target','Prediction'],loc='lower right')\n",
    "    plt.savefig('Alex_Seq'+ str(iter)+'.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map of Target\n",
    "\n",
    "for iter in range(20):\n",
    "    idx_op={'Fi': [0,1]}\n",
    "    sample = target[3]\n",
    "    tmp = sample[2000+iter,..., idx_op['Fi'][0]]\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(tmp)\n",
    "\n",
    "    #plt.yticks(range(-3,3))\n",
    "    plt.colorbar()\n",
    "    plt.draw()\n",
    "    plt.savefig('Alex_Tar'+ str(iter)+'.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of prediction \n",
    "\n",
    "for iter in range(20):\n",
    "    sample = output[3]\n",
    "    tmp = sample[2000+iter,..., idx_op['Fi'][0]]\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(tmp)\n",
    "    plt.colorbar()\n",
    "    plt.draw()\n",
    "    plt.savefig('Alex_Out'+ str(iter)+'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
